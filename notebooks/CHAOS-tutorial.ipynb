{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.8.16\n",
      "IPython version      : 8.8.0\n",
      "\n",
      "numpy       : 1.23.5\n",
      "pandas      : 1.5.3\n",
      "scipy       : 1.10.1\n",
      "scikit-learn: 1.2.2\n",
      "torch       : 2.0.1\n",
      "rdkit       : 2023.3.1\n",
      "gpytorch    : 1.10\n",
      "matplotlib  : 3.3.2\n",
      "botorch     : 0.8.2.dev9+g7f3aa92f\n",
      "wandb       : 0.15.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p numpy,pandas,scipy,scikit-learn,torch,rdkit,gpytorch,matplotlib,botorch,wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To use the Graphein submodule graphein.protein.features.sequence.embeddings, you need to install: biovec \n",
      "biovec cannot be installed via conda\n",
      "To use the Graphein submodule graphein.protein.visualisation, you need to install: pytorch3d \n",
      "To do so, use the following command: conda install -c pytorch3d pytorch3d\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09/08/23 13:53:19] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> To use the Graphein submodule graphein.protein.meshes, you need to        <a href=\"file:///home/rankovic/miniconda3/envs/additive_bo/lib/python3.8/site-packages/graphein/protein/meshes.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">meshes.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/rankovic/miniconda3/envs/additive_bo/lib/python3.8/site-packages/graphein/protein/meshes.py#29\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">29</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         install: pytorch3d                                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         To do so, use the following command: conda install -c pytorch3d pytorch3d <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09/08/23 13:53:19]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m To use the Graphein submodule graphein.protein.meshes, you need to        \u001b]8;id=728192;file:///home/rankovic/miniconda3/envs/additive_bo/lib/python3.8/site-packages/graphein/protein/meshes.py\u001b\\\u001b[2mmeshes.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=841483;file:///home/rankovic/miniconda3/envs/additive_bo/lib/python3.8/site-packages/graphein/protein/meshes.py#29\u001b\\\u001b[2m29\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         install: pytorch3d                                                        \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         To do so, use the following command: conda install -c pytorch3d pytorch3d \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from chaos.data.module import BaseDataModule, Featurizer\n",
    "from chaos.bo.module import BoModule\n",
    "from chaos.initialization.initializers import BOInitializer\n",
    "from pytorch_lightning import seed_everything\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from chaos.utils import instantiate_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAOS Tutorial\n",
    "\n",
    "Welcome to CHAOS! (tutorial) We will walk through the entire workflow of the CHAOS framework, starting with loading your dataset from a CSV file to running a Bayesian optimization loop to find the optimal experimental settings for your chemical reactions.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "CHAOS, which stands for CHemical Additives Optimization Screening is an open-source framework that leverages Bayesian optimization and machine learning to facilitate the optimization of chemical reactions. The objective of this tutorial is to provide a hands-on guide to using CHAOS for chemical reaction optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and featurization\n",
    "\n",
    "CHAOS provides an easy way to featurize molecules or reactions using a variety or representations. Depending on the type of the task (molecular vs reaction optimization) you can set up the Featurizer. In the following cells we demonstrate how to set up a featurizer using DRFP representations for reaction optimization, or fragprints for molecular optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fragprints\n",
    "featurizer = Featurizer(\n",
    "    nBits=512, bond_radius=3, representation=\"fragprints\", task=\"molecular_optimization\"\n",
    ")\n",
    "## DRFP\n",
    "featurizer = Featurizer(\n",
    "    nBits=512, bond_radius=7, representation=\"drfp\", task=\"reaction_optimization\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to employ a specific initialization, you can use the BOInitializer object. Depending on the method, it will provide selection using either clustering, maxmin strategy or random sampling. We show how to set each of these options. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Kmeans\n",
    "initializer = BOInitializer(method=\"kmeans\", n_clusters=10, use_pca=10)\n",
    "## Kmedoids\n",
    "initializer = BOInitializer(method=\"kmedoids\", n_clusters=10, metric=\"jaccard\")\n",
    "## MaxMin\n",
    "initializer = BOInitializer(method=\"maxmin\", n_clusters=10, metric=\"jaccard\")\n",
    "## Random\n",
    "initializer = BOInitializer(method=\"true_random\", n_clusters=10, metric=\"jaccard\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can employ the data module, responsible for storing the featurized data, preprocessing it and splitting into the train and heldout sets. It takes as input the path to a .csv file containing smiles of the reaction components to be screened. The *input_column* is the column of the .csv file that contains the specific component for optimization. For molecular optimization, as in the paper, this would be a column where the set of additive smiles are stored. For reaction representation, the column needs to contain reaction smiles. In the case of OHE you can forward a list of columns, and the featurization would create a unifying OHE vector based on those columns.\n",
    "\n",
    "The target column is the column containing the objective values. Additionally it takes as input initializer and featurizer objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[653, 394, 545, 422, 171, 654, 446, 522, 353, 374] selected reactions\n",
      "Selected reactions: [653, 394, 545, 422, 171, 654, 446, 522, 353, 374]\n"
     ]
    }
   ],
   "source": [
    "dm = BaseDataModule(\n",
    "    data_path=\"../data/additives/additive_rxn_screening_plate_1.csv\",\n",
    "    input_column=\"rxn\",\n",
    "    target_column=\"objective\",\n",
    "    initializer=initializer,\n",
    "    featurizer=featurizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 512]),\n",
       " torch.Size([10, 1]),\n",
       " torch.Size([710, 512]),\n",
       " torch.Size([710, 1]),\n",
       " torch.Size([720, 512]),\n",
       " torch.Size([720, 1]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.train_x.shape, dm.train_y.shape, dm.heldout_x.shape, dm.heldout_y.shape, dm.x.shape, dm.y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the surrogate model\n",
    "\n",
    "The surrogate model can be initialized based on a formatted config defining the necessary arguments like the specific kernel, noise constraints and whether to standardize the output. If you want to change the kernel, just put edit the path to the kernel class, for example *gpytorch.kernels.LinearKernel* or custom kernels, for example from GauChe, you can use them here. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"class_path\": \"chaos.surrogate_models.gp.SimpleGP\",\n",
    "    \"init_args\": {\n",
    "        \"likelihood\": {\n",
    "            \"class_path\": \"gpytorch.likelihoods.GaussianLikelihood\",\n",
    "        },\n",
    "        \"covar_module\": {\n",
    "            \"class_path\": \"gpytorch.kernels.ScaleKernel\",\n",
    "            \"init_args\": {\n",
    "                \"base_kernel\": {\n",
    "                    \"class_path\": \"gpytorch.kernels.MaternKernel\",\n",
    "                    \"init_args\": {\"eps\": 1.0e-06, \"nu\": 0.5},\n",
    "                },\n",
    "                \"eps\": 1.0e-06,\n",
    "            },\n",
    "        },\n",
    "        \"standardize\": True,\n",
    "        \"normalize\": False,\n",
    "        \"initial_noise_val\": 0.0001,\n",
    "        \"noise_constraint\": 1.0e-05,\n",
    "        \"initial_outputscale_val\": 2.0,\n",
    "        \"initial_lengthscale_val\": 0.5,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = instantiate_class(model_config, train_x=dm.train_x, train_y=dm.train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the BO loop\n",
    "The main module for Bayesian optimization is BoModule with primary inputs like the data and the model_config. We can use it inside the pytorch lightning Trainer object to run the BO iterations. All the metrics are saved to wandb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_module = BoModule(\n",
    "        data=dm,\n",
    "        model_config=model_config,\n",
    "        enable_plotting=True,\n",
    "        enable_logging_images=True,\n",
    "        beta=0.1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbojana-rankovic\u001b[0m (\u001b[33mliac\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20230908_135404-sgp5qg43</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/liac/chaos-tutorial/runs/sgp5qg43' target=\"_blank\">jolly-aardvark-2</a></strong> to <a href='https://wandb.ai/liac/chaos-tutorial' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/liac/chaos-tutorial' target=\"_blank\">https://wandb.ai/liac/chaos-tutorial</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/liac/chaos-tutorial/runs/sgp5qg43' target=\"_blank\">https://wandb.ai/liac/chaos-tutorial/runs/sgp5qg43</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/rankovic/miniconda3/envs/additive_bo/lib/python3.8/site-packages/pytorch_lightning/trainer/setup.py:176: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/home/rankovic/miniconda3/envs/additive_bo/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:68: UserWarning: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\n",
      "/home/rankovic/miniconda3/envs/additive_bo/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py:171: UserWarning: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "------------------------------\n",
      "0         Trainable params\n",
      "0         Non-trainable params\n",
      "0         Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eec625f06674dc1a8f32ad0db208ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rankovic/miniconda3/envs/additive_bo/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('suggestion_idx', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5ee8d2365d466da9f0e11191005f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='13.553 MB of 13.553 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>MAE_all</td><td>█▄▂▁▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▃▂</td></tr><tr><td>MAE_bottom_5</td><td>▁▄▆▇▆▆▇▆▇█▇▇▇▇▆▅▅▅▅▆▆▆▆▆▆▅▆▆▆▆▆▆▆▆▇▇▆▆▇▆</td></tr><tr><td>MAE_top_5</td><td>█▅▃▂▃▃▂▃▂▁▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▃▂▂▂▂▂▂▃▂▂▂</td></tr><tr><td>NLPD_all</td><td>█▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>NLPD_bottom_5</td><td>█▄▁▁▁▁▂▂▃▄▃▄▄▄▄▃▄▄▄▄▄▅▄▄▄▄▅▅▆▅▆▆▆▇▇▇▇▆▆▅</td></tr><tr><td>NLPD_top_5</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>R2_all</td><td>▁▆████▇▇▇▆▇▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆▆▆▆▆▆▆▆</td></tr><tr><td>R2_bottom_5</td><td>█▇▅▅▅▄▄▄▄▃▄▄▅▄▄▅▅▅▅▅▅▄▄▄▄▄▄▄▂▃▂▂▂▂▂▂▁▃▂▄</td></tr><tr><td>R2_top_5</td><td>▃▅▇▇▇▇█▇██▇▇▇▇▇▇▆▇▆▇▇▇▇▇▇▇▆▆▆▆▆▆▆▆▆▆▆▄▁▁</td></tr><tr><td>average_similarity</td><td>▁▁▄▆▆▇▇▇██████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇▇▇▇▇▇▇▇</td></tr><tr><td>covar_module.base_kernel.lengthscale</td><td>▁▂▄▆▄▅▆▅▆█▅▆▅▅▄▄▄▃▃▃▃▄▄▄▄▄▃▃▄▄▄▄▄▄▄▄▄▄▄▃</td></tr><tr><td>covar_module.outputscale</td><td>▁▃▃▅▄▅▆▅▆█▆▆▆▆▅▅▅▅▄▄▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▅▆▅</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>likelihood.noise_covar.noise</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_module.constant</td><td>█▇▅▄▄▃▃▃▂▂▂▂▂▂▃▃▃▃▃▃▃▂▂▂▂▃▂▂▂▂▂▂▂▁▁▂▂▂▂▃</td></tr><tr><td>suggestion_idx</td><td>▂▁▆▅▄▂▃▁▄▁▃▁▆▂▄▄▂▆▃▃▃▆▂▄▆▃▁▂▁▁▁█▇▅▇▄▄▁▂▄</td></tr><tr><td>sum_acq_values</td><td>▁▅▇█▇▇▇▇▇█▇▇▇▇▆▆▆▆▅▆▆▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▄▄▅▄</td></tr><tr><td>train/best_so_far</td><td>▁▂▂▂▂▂▃▃▃▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████████████</td></tr><tr><td>train/mae</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/mse</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/msll</td><td>█▁▁▁▁▁▁▂▁▁▁▁▁▁▂▁▁▂▂▁▁▁▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▃▁</td></tr><tr><td>train/nlpd</td><td>█▁▁▁▁▁▁▂▁▁▁▁▁▁▂▁▁▂▂▁▁▁▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▂▁</td></tr><tr><td>train/qce</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/r2</td><td>▁███████████████████████████████████████</td></tr><tr><td>train/suggestion</td><td>▆▆▆▆▅▅▃▅▅▁▅▄▆▅▅▁▅▂▆▆▆▁▅▁▄▆▅▅▅▅▆▄▅▅▁▆█▅▅▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>valid/mae</td><td>█▄▂▁▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▃▂</td></tr><tr><td>valid/mse</td><td>█▃▁▁▁▁▂▂▂▃▂▂▂▂▂▁▂▁▁▁▂▂▂▂▂▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>valid/msll</td><td>█▇▄▂▃▂▁▂▁▂▃▃▄▄▄▄▅▄▄▄▄▃▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▄▅▅</td></tr><tr><td>valid/nlpd</td><td>█▂▁▂▁▁▂▂▂▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁</td></tr><tr><td>valid/qce</td><td>▆▇▇▄▇▆▃▄▁▁▅▄▆▅▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▆▆▆▆▅▆▆███</td></tr><tr><td>valid/r2</td><td>▁▆████▇▇▇▆▇▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>MAE_all</td><td>19082.87983</td></tr><tr><td>MAE_bottom_5</td><td>30802.31542</td></tr><tr><td>MAE_top_5</td><td>28353.44834</td></tr><tr><td>NLPD_all</td><td>11.35638</td></tr><tr><td>NLPD_bottom_5</td><td>11.54814</td></tr><tr><td>NLPD_top_5</td><td>11.4898</td></tr><tr><td>R2_all</td><td>0.0181</td></tr><tr><td>R2_bottom_5</td><td>-33366.19609</td></tr><tr><td>R2_top_5</td><td>-35.51188</td></tr><tr><td>average_similarity</td><td>0.49711</td></tr><tr><td>covar_module.base_kernel.lengthscale</td><td>4.2593</td></tr><tr><td>covar_module.outputscale</td><td>1.37109</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>input_stat_feature_dimension</td><td>512</td></tr><tr><td>input_stat_n_points</td><td>720</td></tr><tr><td>likelihood.noise_covar.noise</td><td>1e-05</td></tr><tr><td>mean_module.constant</td><td>-0.85492</td></tr><tr><td>quantile_75_count</td><td>47</td></tr><tr><td>quantile_90_count</td><td>18</td></tr><tr><td>quantile_95_count</td><td>12</td></tr><tr><td>quantile_99_count</td><td>5</td></tr><tr><td>suggestion_idx</td><td>240.0</td></tr><tr><td>sum_acq_values</td><td>24756995.53936</td></tr><tr><td>target_stat_max</td><td>86045.92668</td></tr><tr><td>target_stat_mean</td><td>31774.73684</td></tr><tr><td>target_stat_std</td><td>22052.69712</td></tr><tr><td>target_stat_var</td><td>486321450.13971</td></tr><tr><td>top_10_count</td><td>7</td></tr><tr><td>top_1_count</td><td>1</td></tr><tr><td>top_3_count</td><td>3</td></tr><tr><td>top_5_count</td><td>4</td></tr><tr><td>train/best_so_far</td><td>86045.92668</td></tr><tr><td>train/mae</td><td>0.21728</td></tr><tr><td>train/mse</td><td>0.0751</td></tr><tr><td>train/msll</td><td>5.58736</td></tr><tr><td>train/nlpd</td><td>5.58736</td></tr><tr><td>train/qce</td><td>0.05</td></tr><tr><td>train/r2</td><td>1.0</td></tr><tr><td>train/suggestion</td><td>12941.253</td></tr><tr><td>trainer/global_step</td><td>99</td></tr><tr><td>valid/mae</td><td>19082.87983</td></tr><tr><td>valid/mse</td><td>453251328.73224</td></tr><tr><td>valid/msll</td><td>11.1723</td></tr><tr><td>valid/nlpd</td><td>11.35638</td></tr><tr><td>valid/qce</td><td>0.04836</td></tr><tr><td>valid/r2</td><td>0.0181</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">jolly-aardvark-2</strong> at: <a href='https://wandb.ai/liac/chaos-tutorial/runs/sgp5qg43' target=\"_blank\">https://wandb.ai/liac/chaos-tutorial/runs/sgp5qg43</a><br/>Synced 6 W&B file(s), 200 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230908_135404-sgp5qg43/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger = (\n",
    "    WandbLogger(project=\"chaos-tutorial\") if bo_module.enable_plotting else None\n",
    ")\n",
    "trainer = Trainer(\n",
    "    max_epochs=100,\n",
    "    logger=logger,\n",
    "    log_every_n_steps=1,\n",
    "    num_sanity_val_steps=0,\n",
    "    min_epochs=1,\n",
    "    max_steps=-1,\n",
    "    accelerator=\"cpu\",\n",
    "    devices=1,\n",
    ")\n",
    "trainer.fit(bo_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "additive_bo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
